 28%|██████████████████████████████████████████▌                                                                                                           | 17/60 [01:30<03:58,  5.54s/it]Traceback (most recent call last):
{'loss': 1.8302, 'grad_norm': 0.48997004594052723, 'learning_rate': 0.00019666666666666666, 'epoch': 0.15}
{'loss': 1.7663, 'grad_norm': 0.8574126895740474, 'learning_rate': 0.00019333333333333333, 'epoch': 0.31}
{'loss': 1.6188, 'grad_norm': 0.30135552829316686, 'learning_rate': 0.00019, 'epoch': 0.46}
{'loss': 1.5294, 'grad_norm': 0.2709941572774166, 'learning_rate': 0.0001866666666666667, 'epoch': 0.62}
{'loss': 1.5103, 'grad_norm': 0.12393142170217035, 'learning_rate': 0.00018333333333333334, 'epoch': 0.77}
Invalidate trace cache @ step 1126: expected module 1, but got module 2332
  File "/home/long/WhatMakesAGoodPrompt/finetuning-codes/customed_run_sft.py", line 280, in <module>                                                                                       
{'eval_loss': 1.4471412897109985, 'eval_runtime': 2.5853, 'eval_samples_per_second': 38.293, 'eval_steps_per_second': 0.774, 'epoch': 0.77}
Invalidate trace cache @ step 1126: expected module 2332, but got module 1
[2025-05-22 17:56:05,896] [WARNING] [stage3.py:2114:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.4468, 'grad_norm': 0.10820708528096636, 'learning_rate': 0.00018, 'epoch': 0.92}
{'loss': 2.1055, 'grad_norm': 0.18685086836982978, 'learning_rate': 0.00017666666666666666, 'epoch': 1.08}
{'loss': 1.3885, 'grad_norm': 0.11854488104623595, 'learning_rate': 0.00017333333333333334, 'epoch': 1.23}
{'loss': 1.3447, 'grad_norm': 0.09924233979878597, 'learning_rate': 0.00017, 'epoch': 1.38}
{'loss': 1.305, 'grad_norm': 0.08737859019116699, 'learning_rate': 0.0001666666666666667, 'epoch': 1.54}
Invalidate trace cache @ step 1126: expected module 1, but got module 2332
{'eval_loss': 1.3001883029937744, 'eval_runtime': 1.4744, 'eval_samples_per_second': 67.147, 'eval_steps_per_second': 1.356, 'epoch': 1.54}
Invalidate trace cache @ step 1126: expected module 2332, but got module 1
{'loss': 1.2988, 'grad_norm': 0.07951345361207364, 'learning_rate': 0.00016333333333333334, 'epoch': 1.69}
{'loss': 1.3128, 'grad_norm': 0.06333935621945995, 'learning_rate': 0.00016, 'epoch': 1.85}
{'loss': 1.9147, 'grad_norm': 0.1005942901535369, 'learning_rate': 0.00015666666666666666, 'epoch': 2.0}
{'loss': 1.2798, 'grad_norm': 0.056399644942041714, 'learning_rate': 0.00015333333333333334, 'epoch': 2.15}
{'loss': 1.296, 'grad_norm': 0.05317719264257038, 'learning_rate': 0.00015000000000000001, 'epoch': 2.31}
Invalidate trace cache @ step 1126: expected module 1, but got module 2332
{'eval_loss': 1.2676823139190674, 'eval_runtime': 1.4796, 'eval_samples_per_second': 66.909, 'eval_steps_per_second': 1.352, 'epoch': 2.31}
Invalidate trace cache @ step 1126: expected module 2332, but got module 1
{'loss': 1.2685, 'grad_norm': 0.06335736844000547, 'learning_rate': 0.00014666666666666666, 'epoch': 2.46}
{'loss': 1.2495, 'grad_norm': 0.04754069247508135, 'learning_rate': 0.00014333333333333334, 'epoch': 2.62}
    main()
  File "/home/long/WhatMakesAGoodPrompt/finetuning-codes/customed_run_sft.py", line 276, in main
    train_function(model_args, script_args, training_args)
  File "/home/long/WhatMakesAGoodPrompt/finetuning-codes/customed_run_sft.py", line 233, in train_function
    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/transformers/trainer.py", line 2114, in train
    return inner_training_loop(
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/transformers/trainer.py", line 3612, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/accelerate/accelerator.py", line 2233, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2259, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/long/WhatMakesAGoodPrompt/finetuning-codes/customed_run_sft.py", line 280, in <module>
[rank0]:     main()
[rank0]:   File "/home/long/WhatMakesAGoodPrompt/finetuning-codes/customed_run_sft.py", line 276, in main
[rank0]:     train_function(model_args, script_args, training_args)
[rank0]:   File "/home/long/WhatMakesAGoodPrompt/finetuning-codes/customed_run_sft.py", line 233, in train_function
[rank0]:     train_result = trainer.train(resume_from_checkpoint=last_checkpoint)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/transformers/trainer.py", line 2114, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/transformers/trainer.py", line 3612, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/accelerate/accelerator.py", line 2233, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2259, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/long/.conda/envs/good_prompt/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
